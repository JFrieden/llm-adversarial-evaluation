### llm-adversarial-evaluation

This repository contains the work for my COM S 6730-3, "Advanced Topics In Machine Learning: Secure AI" term project. This project, **Benchmarking Adversarial Attacks on LLMs** aims to both provide a comparative analysis of different LLMs (gpt-4o-mini, claude-3.5 haiku, and llama-3-3-70b-instruct) against various jailbreaking methods.

Our prompts come from red teaming efforts for chatGPT-4 discussed in [a 2024 OpenAI report](https://arxiv.org/pdf/2303.08774) and our jailbreaking techniques come from a variety of sources, but are largely informed by [Wei et al. (2023)](https://arxiv.org/pdf/2307.02483).


