### llm-adversarial-evaluation

This repository contains the work for my COM S 6730-3, "Advanced Topics In Machine Learning: Secure AI" term project. This project, **Benchmarking Adversarial Attacks on Open-Source LLMs** aims to both provide a comparative analysis of LLaMA, GPT-2, BERT, and T5 under various adversarial attacks, and to offer me the opportunity to learn more about Prompt Injection, Adversarial Text Perturbation, and Text-Based Adversarial Examples. If time permits, I'll also try to address Model Inversion and Poisoning in fine-tuning if time allows.
